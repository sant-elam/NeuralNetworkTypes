# -*- coding: utf-8 -*-
"""FeedForwardNeuralNetwork.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xD_TVCSxs_4tFJihoSlm8akIMUQngLJQ

Load the necessary libraries
"""

import torch
import torch.nn as nn
import torchvision.datasets as dataset
import torchvision.transforms as transforms
import torch.utils.data as data
import torch.optim as optim

"""Load the dataset"""

train_data = dataset.MNIST(root="./data",
                           train=True,
                           transform=transforms.ToTensor(),
                           download=True)

test_data = dataset.MNIST(root="./data",
                          train=False,
                          transform=transforms.ToTensor(),
                          download=True)

BATCH_SIZE = 64
NUM_OF_EPOCHS = 50

train_dataloader = data.DataLoader(dataset=train_data,
                              batch_size=BATCH_SIZE,
                              shuffle=True)

test_dataloader = data.DataLoader(dataset=test_data,
                             batch_size=BATCH_SIZE,
                             shuffle=True)

"""Create the FeedForwardNeuralNetwork"""

class FeedForwardNeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden01_size, hidden02_size, num_classes):
        super(FeedForwardNeuralNetwork, self).__init__()

        self.linear1 = nn.Linear(input_size, hidden01_size)
        self.relu1 = nn.ReLU()

        self.linear2 = nn.Linear(hidden01_size, hidden02_size)
        self.relu2 = nn.ReLU()

        self.output = nn.Linear(hidden02_size, num_classes)

    def forward(self, x):
        out = self.linear1(x)
        out = self.relu2(out)

        out = self.linear2(out)
        out = self.relu2(out)

        out = self.output(out)

        return out

INPUT_SIZE = 28*28
HIDDEN01_SIZE = 100
HIDDEN02_SIZE = 50
OUTPUT_SIZE = 10

LEARNING_RATE = 0.1

model = FeedForwardNeuralNetwork(INPUT_SIZE, HIDDEN01_SIZE, HIDDEN02_SIZE, OUTPUT_SIZE)

if torch.cuda.is_available():
  device = "cuda:0"
else:
  device = "cpu"

model = model.to(device)

loss_function = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)

"""TRAINING"""

for epoch in range(NUM_OF_EPOCHS):
  step = 0

  print(f"--------Starting Epoch {epoch+1}-----------")

  for i, (images, labels) in enumerate(train_dataloader):

    # Reshape the image tensor
    # requires_grad will track all gradients for backward propagation
    images = images.view(-1, 28*28).requires_grad_().to(device)

    # Reset the gradients of all model parameters
    optimizer.zero_grad()

    # Forward pass or forward propagation
    outputs = model(images)

    # Compute loss
    loss = loss_function(outputs, labels)

    # Backward pass or backward propagation
    loss.backward()

    # Update the model parameter - weight and bias
    optimizer.step()

    step +=1

    # Check for model accuracy at each 100 step
    step_to_check = 100

    if step % step_to_check == 0:
      correct_prediction =0
      total_count = 0
      for images, labels in test_dataloader:
          images = images.view(-1, 28*28).requires_grad_().to(device)

          outputs = model(images)

          # Find the indices of the maximum values along the specified dimension
          _, predicted = torch.max(outputs.data, 1)

          # Total number of dataset used
          total_count += len(labels)

          # Calculate the number of correct predictions
          correct_output = (predicted==labels).sum()

          # Add correct predictions to the total correct predictions
          correct_prediction += correct_output

      # Finding accuracy
      model_accuracy = (correct_prediction/total_count) * 100.0
      print(f" Epoch {epoch+1}  Step {step} Accuracy {model_accuracy:.2f}. Loss {loss.item():.2f}")

